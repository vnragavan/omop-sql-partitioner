#!/usr/bin/env python3
"""
Cleanup utility for OMOP Partitioner generated files
"""

import os
import shutil
import argparse
import logging
from pathlib import Path
from typing import List, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OMOPCleanup:
    """Handles cleanup of OMOP partitioner generated files"""
    
    def __init__(self, base_dir: str = "."):
        self.base_dir = Path(base_dir)
        self.default_dirs = [
            "sql_exports",
            "output", 
            "partitions",
            "temp_exports"
        ]
        
        # File patterns for files that are REGENERATED by OMOP partitioner scripts
        # These are the only files that should be cleaned up
        self.file_patterns = [
            "partition_*_complete.sql",  # Generated partition files
            "schema.sql",                # Generated schema export
            "source_graph.dot",          # Generated source graph
            "source_graph.png",          # Generated source graph image
            "partition_*_graph.dot",     # Generated partition graphs
            "partition_*_graph.png",     # Generated partition graph images
            "omop_partitions_config_*.zip", # Generated config archives
            "partitioning_report.txt",   # Generated partitioning report
            "import_partitions.sh",      # Generated import script
            "temp_*",                    # Temporary files
        ]
        
        # Directories to exclude from scanning
        self.exclude_dirs = {
            ".venv", "__pycache__", ".git", "node_modules", 
            ".pytest_cache", "build", "dist", ".tox"
        }
        
        # Project directories that contain static files (not generated)
        self.project_dirs = {
            "ddl", "docs", "examples", "tests", "config"
        }
    
    def list_generated_files(self, output_dir: Optional[str] = None) -> List[Path]:
        """List all generated files that can be cleaned up"""
        files_to_clean = []
        
        # Check default directories
        for dir_name in self.default_dirs:
            dir_path = self.base_dir / dir_name
            if dir_path.exists():
                files_to_clean.extend(dir_path.rglob("*"))
        
        # Check custom output directory
        if output_dir:
            custom_dir = Path(output_dir)
            if custom_dir.exists():
                files_to_clean.extend(custom_dir.rglob("*"))
        
        # Scan for pattern-based files in the entire base directory
        for pattern in self.file_patterns:
            try:
                pattern_files = list(self.base_dir.rglob(pattern))
                # Filter out files in excluded directories and project directories
                filtered_files = []
                for file_path in pattern_files:
                    # Check if any parent directory is in exclude list or project directories
                    should_exclude = False
                    for parent in file_path.parents:
                        if parent.name in self.exclude_dirs or parent.name in self.project_dirs:
                            should_exclude = True
                            break
                    if not should_exclude:
                        filtered_files.append(file_path)
                files_to_clean.extend(filtered_files)
            except Exception as e:
                logger.debug(f"Error scanning for pattern {pattern}: {e}")
        
        # Filter out directories and only include files
        files_only = [f for f in files_to_clean if f.is_file()]
        
        # Remove duplicates while preserving order
        seen = set()
        unique_files = []
        for file_path in files_only:
            if file_path not in seen:
                seen.add(file_path)
                unique_files.append(file_path)
        
        return unique_files
    
    def scan_for_omop_files(self) -> List[Path]:
        """Scan for OMOP partitioner files using common naming patterns"""
        omop_files = []
        
        # Common OMOP partitioner file patterns (only REGENERATED files)
        omop_patterns = [
            "omop_partitions_config_*.zip",    # Generated config archives
            "partition_*_complete.sql",        # Generated complete partition files
            "partition_*_graph.dot",           # Generated partition graphs
            "partition_*_graph.png",           # Generated partition graph images
            "source_graph.dot",                # Generated source graph
            "source_graph.png",                # Generated source graph image
            "partitioning_report.txt",         # Generated partitioning report
            "import_partitions.sh",            # Generated import script
            "schema.sql",                      # Generated schema export (in output dirs)
        ]
        
        for pattern in omop_patterns:
            try:
                pattern_files = list(self.base_dir.rglob(pattern))
                # Filter out files in excluded directories and project directories
                filtered_files = []
                for file_path in pattern_files:
                    # Check if any parent directory is in exclude list or project directories
                    should_exclude = False
                    for parent in file_path.parents:
                        if parent.name in self.exclude_dirs or parent.name in self.project_dirs:
                            should_exclude = True
                            break
                    if not should_exclude:
                        filtered_files.append(file_path)
                omop_files.extend(filtered_files)
            except Exception as e:
                logger.debug(f"Error scanning for OMOP pattern {pattern}: {e}")
        
        # Filter out directories and only include files
        files_only = [f for f in omop_files if f.is_file()]
        
        # Remove duplicates
        seen = set()
        unique_files = []
        for file_path in files_only:
            if file_path not in seen:
                seen.add(file_path)
                unique_files.append(file_path)
        
        return unique_files
    
    def comprehensive_scan(self, output_dir: Optional[str] = None) -> dict:
        """Perform comprehensive scan for all OMOP partitioner related files"""
        results = {
            "default_dirs": [],
            "pattern_files": [],
            "omop_files": [],
            "custom_dir": [],
            "all_files": [],
            "total_size": 0
        }
        
        # Scan default directories
        for dir_name in self.default_dirs:
            dir_path = self.base_dir / dir_name
            if dir_path.exists():
                dir_files = list(dir_path.rglob("*"))
                dir_files = [f for f in dir_files if f.is_file()]
                results["default_dirs"].extend(dir_files)
        
        # Scan for pattern-based files
        results["pattern_files"] = self.list_generated_files(output_dir)
        
        # Scan for OMOP-specific files
        results["omop_files"] = self.scan_for_omop_files()
        
        # Scan custom output directory
        if output_dir:
            custom_dir = Path(output_dir)
            if custom_dir.exists():
                custom_files = list(custom_dir.rglob("*"))
                custom_files = [f for f in custom_files if f.is_file()]
                results["custom_dir"] = custom_files
        
        # Combine all files and remove duplicates
        all_files = []
        seen = set()
        
        for file_list in [results["default_dirs"], results["pattern_files"], 
                         results["omop_files"], results["custom_dir"]]:
            for file_path in file_list:
                if file_path not in seen:
                    seen.add(file_path)
                    all_files.append(file_path)
        
        results["all_files"] = all_files
        results["total_size"] = sum(f.stat().st_size for f in all_files if f.exists())
        
        return results
    
    def get_directory_sizes(self, output_dir: Optional[str] = None) -> dict:
        """Get sizes of directories containing generated files"""
        sizes = {}
        
        # Check default directories
        for dir_name in self.default_dirs:
            dir_path = self.base_dir / dir_name
            if dir_path.exists():
                total_size = sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file())
                sizes[str(dir_path)] = total_size
        
        # Check custom output directory
        if output_dir:
            custom_dir = Path(output_dir)
            if custom_dir.exists():
                total_size = sum(f.stat().st_size for f in custom_dir.rglob('*') if f.is_file())
                sizes[str(custom_dir)] = total_size
        
        # Also check for any directories that contain generated files (like my-partitions)
        # This ensures we catch custom output directories from previous runs
        generated_files = self.list_generated_files(output_dir)
        for file_path in generated_files:
            if file_path.exists():
                dir_path = file_path.parent
                dir_str = str(dir_path)
                if dir_str not in sizes:
                    # Calculate size for this directory
                    dir_files = [f for f in dir_path.rglob('*') if f.is_file()]
                    total_size = sum(f.stat().st_size for f in dir_files)
                    sizes[dir_str] = total_size
        
        return sizes
    
    def format_size(self, size_bytes: int) -> str:
        """Format file size in human readable format"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"
    
    def cleanup_files(self, 
                     output_dir: Optional[str] = None,
                     dry_run: bool = False,
                     confirm: bool = False) -> dict:
        """Clean up generated files"""
        files_to_clean = self.list_generated_files(output_dir)
        sizes = self.get_directory_sizes(output_dir)
        
        total_size = sum(sizes.values())
        total_files = len(files_to_clean)
        
        logger.info(f"Found {total_files} files to clean up")
        logger.info(f"Total size: {self.format_size(total_size)}")
        
        if total_files == 0:
            logger.info("No files to clean up")
            return {"files_removed": 0, "space_freed": 0}
        
        # Show what will be cleaned
        logger.info("\nDirectories and sizes:")
        for dir_path, size in sizes.items():
            logger.info(f"  {dir_path}: {self.format_size(size)}")
        
        logger.info(f"\nFiles to be removed:")
        for file_path in files_to_clean[:10]:  # Show first 10 files
            logger.info(f"  {file_path}")
        if len(files_to_clean) > 10:
            logger.info(f"  ... and {len(files_to_clean) - 10} more files")
        
        if dry_run:
            logger.info(f"\n[DRY RUN] Would remove {total_files} files ({self.format_size(total_size)})")
            return {"files_removed": 0, "space_freed": total_size}
        
        # Confirm deletion
        if not confirm:
            response = input(f"\nAre you sure you want to delete {total_files} files ({self.format_size(total_size)})? [y/N]: ")
            if response.lower() not in ['y', 'yes']:
                logger.info("Cleanup cancelled")
                return {"files_removed": 0, "space_freed": 0}
        
        # Perform cleanup
        files_removed = 0
        space_freed = 0
        
        try:
            # Remove files
            for file_path in files_to_clean:
                try:
                    file_size = file_path.stat().st_size
                    file_path.unlink()
                    files_removed += 1
                    space_freed += file_size
                except Exception as e:
                    logger.warning(f"Failed to remove {file_path}: {e}")
            
            # Remove empty directories
            for dir_name in self.default_dirs:
                dir_path = self.base_dir / dir_name
                if dir_path.exists() and not any(dir_path.iterdir()):
                    try:
                        dir_path.rmdir()
                        logger.info(f"Removed empty directory: {dir_path}")
                    except Exception as e:
                        logger.warning(f"Failed to remove directory {dir_path}: {e}")
            
            # Remove custom output directory if empty
            if output_dir:
                custom_dir = Path(output_dir)
                if custom_dir.exists() and not any(custom_dir.iterdir()):
                    try:
                        custom_dir.rmdir()
                        logger.info(f"Removed empty directory: {custom_dir}")
                    except Exception as e:
                        logger.warning(f"Failed to remove directory {custom_dir}: {e}")
            
            logger.info(f"\nâœ… Cleanup completed!")
            logger.info(f"Files removed: {files_removed}")
            logger.info(f"Space freed: {self.format_size(space_freed)}")
            
            return {"files_removed": files_removed, "space_freed": space_freed}
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
            return {"files_removed": files_removed, "space_freed": space_freed}
    
    def cleanup_specific_directory(self, directory: str, dry_run: bool = False, confirm: bool = False) -> dict:
        """Clean up a specific directory"""
        dir_path = Path(directory)
        
        if not dir_path.exists():
            logger.warning(f"Directory does not exist: {directory}")
            return {"files_removed": 0, "space_freed": 0}
        
        files_to_clean = list(dir_path.rglob("*"))
        files_only = [f for f in files_to_clean if f.is_file()]
        
        total_size = sum(f.stat().st_size for f in files_only)
        total_files = len(files_only)
        
        logger.info(f"Directory: {directory}")
        logger.info(f"Files to clean: {total_files}")
        logger.info(f"Total size: {self.format_size(total_size)}")
        
        if total_files == 0:
            logger.info("No files to clean up")
            return {"files_removed": 0, "space_freed": 0}
        
        if dry_run:
            logger.info(f"[DRY RUN] Would remove {total_files} files ({self.format_size(total_size)})")
            return {"files_removed": 0, "space_freed": total_size}
        
        # Confirm deletion
        if not confirm:
            response = input(f"\nAre you sure you want to delete {total_files} files from {directory} ({self.format_size(total_size)})? [y/N]: ")
            if response.lower() not in ['y', 'yes']:
                logger.info("Cleanup cancelled")
                return {"files_removed": 0, "space_freed": 0}
        
        # Perform cleanup
        files_removed = 0
        space_freed = 0
        
        try:
            # Remove all files in directory
            for file_path in files_only:
                try:
                    file_size = file_path.stat().st_size
                    file_path.unlink()
                    files_removed += 1
                    space_freed += file_size
                except Exception as e:
                    logger.warning(f"Failed to remove {file_path}: {e}")
            
            # Remove the directory itself
            try:
                dir_path.rmdir()
                logger.info(f"Removed directory: {directory}")
            except Exception as e:
                logger.warning(f"Failed to remove directory {directory}: {e}")
            
            logger.info(f"\nâœ… Cleanup completed!")
            logger.info(f"Files removed: {files_removed}")
            logger.info(f"Space freed: {self.format_size(space_freed)}")
            
            return {"files_removed": files_removed, "space_freed": space_freed}
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
            return {"files_removed": files_removed, "space_freed": space_freed}

def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        description="Clean up OMOP Partitioner generated files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # List files that would be cleaned
  omop-cleanup --list

  # Comprehensive scan for all OMOP files
  omop-cleanup --list --comprehensive

  # Dry run (show what would be cleaned)
  omop-cleanup --dry-run

  # Clean up all generated files
  omop-cleanup

  # Clean up specific directory
  omop-cleanup --directory sql_exports

  # Clean up with confirmation
  omop-cleanup --confirm

  # Clean up custom output directory
  omop-cleanup --output-dir my_partitions
        """
    )
    
    parser.add_argument(
        "--list", "-l",
        action="store_true",
        help="List files that would be cleaned up"
    )
    
    parser.add_argument(
        "--comprehensive", "-c",
        action="store_true",
        help="Perform comprehensive scan for all OMOP-related files"
    )
    
    parser.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="Show what would be cleaned without actually deleting"
    )
    
    parser.add_argument(
        "--confirm", "-y",
        action="store_true",
        help="Skip confirmation prompt"
    )
    
    parser.add_argument(
        "--directory", "-d",
        help="Clean up specific directory"
    )
    
    parser.add_argument(
        "--output-dir",
        help="Clean up custom output directory"
    )
    
    parser.add_argument(
        "--base-dir",
        default=".",
        help="Base directory to search for files (default: current directory)"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose logging"
    )
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        cleanup = OMOPCleanup(args.base_dir)
        
        if args.list:
            if args.comprehensive:
                # Comprehensive scan
                results = cleanup.comprehensive_scan(args.output_dir)
                
                print(f"\nðŸ” Comprehensive OMOP Files Scan:")
                print(f"Total files found: {len(results['all_files'])}")
                print(f"Total size: {cleanup.format_size(results['total_size'])}")
                
                print(f"\nðŸ“Š Scan breakdown:")
                print(f"  Default directories: {len(results['default_dirs'])} files")
                print(f"  Pattern-based files: {len(results['pattern_files'])} files")
                print(f"  OMOP-specific files: {len(results['omop_files'])} files")
                if results['custom_dir']:
                    print(f"  Custom directory: {len(results['custom_dir'])} files")
                
                # Show size breakdown by directory
                print(f"\nðŸ“ Size by directory:")
                dir_sizes = {}
                for file_path in results['all_files']:
                    if file_path.exists():
                        dir_name = str(file_path.parent)
                        if dir_name not in dir_sizes:
                            dir_sizes[dir_name] = 0
                        dir_sizes[dir_name] += file_path.stat().st_size
                
                for dir_name, size in sorted(dir_sizes.items()):
                    print(f"  {dir_name}: {cleanup.format_size(size)}")
                
                if results['all_files']:
                    print(f"\nðŸ“„ All files found:")
                    for file_path in results['all_files'][:20]:  # Show first 20 files
                        print(f"  {file_path}")
                    if len(results['all_files']) > 20:
                        print(f"  ... and {len(results['all_files']) - 20} more files")
                else:
                    print("No files found to clean up")
            else:
                # Standard list
                files = cleanup.list_generated_files(args.output_dir)
                sizes = cleanup.get_directory_sizes(args.output_dir)
                
                print(f"\nðŸ“ Generated Files Summary:")
                print(f"Total files: {len(files)}")
                
                total_size = sum(sizes.values())
                print(f"Total size: {cleanup.format_size(total_size)}")
                
                print(f"\nðŸ“Š Directory breakdown:")
                for dir_path, size in sizes.items():
                    print(f"  {dir_path}: {cleanup.format_size(size)}")
                
                if files:
                    print(f"\nðŸ“„ Files to be cleaned:")
                    for file_path in files[:20]:  # Show first 20 files
                        print(f"  {file_path}")
                    if len(files) > 20:
                        print(f"  ... and {len(files) - 20} more files")
                else:
                    print("No files found to clean up")
        
        elif args.directory:
            # Clean specific directory
            result = cleanup.cleanup_specific_directory(
                args.directory, 
                dry_run=args.dry_run, 
                confirm=args.confirm
            )
        
        else:
            # Clean all generated files
            result = cleanup.cleanup_files(
                output_dir=args.output_dir,
                dry_run=args.dry_run,
                confirm=args.confirm
            )
        
    except KeyboardInterrupt:
        logger.info("Operation cancelled by user")
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        if args.verbose:
            logger.exception("Full traceback:")

if __name__ == "__main__":
    main()

